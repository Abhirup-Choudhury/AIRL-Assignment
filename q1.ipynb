{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e2c994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:46:45.284426Z",
     "iopub.status.busy": "2025-10-02T19:46:45.284160Z",
     "iopub.status.idle": "2025-10-02T19:46:55.999950Z",
     "shell.execute_reply": "2025-10-02T19:46:55.999217Z"
    },
    "papermill": {
     "duration": 10.722385,
     "end_time": "2025-10-02T19:46:56.001441",
     "exception": false,
     "start_time": "2025-10-02T19:46:45.279056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import os\n",
    "import csv\n",
    "cwd = os.getcwd()\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e1ea7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:46:56.009529Z",
     "iopub.status.busy": "2025-10-02T19:46:56.009004Z",
     "iopub.status.idle": "2025-10-02T19:46:56.012777Z",
     "shell.execute_reply": "2025-10-02T19:46:56.012058Z"
    },
    "papermill": {
     "duration": 0.008671,
     "end_time": "2025-10-02T19:46:56.013835",
     "exception": false,
     "start_time": "2025-10-02T19:46:56.005164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.optim import Adam,SGD,AdamW\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd58c61e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:46:56.021007Z",
     "iopub.status.busy": "2025-10-02T19:46:56.020607Z",
     "iopub.status.idle": "2025-10-02T19:46:56.024399Z",
     "shell.execute_reply": "2025-10-02T19:46:56.023667Z"
    },
    "papermill": {
     "duration": 0.008431,
     "end_time": "2025-10-02T19:46:56.025402",
     "exception": false,
     "start_time": "2025-10-02T19:46:56.016971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_timestamp():\n",
    "    try:\n",
    "        now = datetime.now(ZoneInfo(\"Asia/Kolkata\"))\n",
    "    except:\n",
    "        print(\"Couldn't get Zone Info...\")\n",
    "        now = datetime.now()\n",
    "    time_str = now.strftime(\"%d_%m_%H_%M\")\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5fcc868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:46:56.032566Z",
     "iopub.status.busy": "2025-10-02T19:46:56.032331Z",
     "iopub.status.idle": "2025-10-02T19:46:56.081882Z",
     "shell.execute_reply": "2025-10-02T19:46:56.081209Z"
    },
    "papermill": {
     "duration": 0.054385,
     "end_time": "2025-10-02T19:46:56.082971",
     "exception": false,
     "start_time": "2025-10-02T19:46:56.028586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35eae5",
   "metadata": {
    "papermill": {
     "duration": 0.003029,
     "end_time": "2025-10-02T19:46:56.089345",
     "exception": false,
     "start_time": "2025-10-02T19:46:56.086316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57141a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:46:56.096816Z",
     "iopub.status.busy": "2025-10-02T19:46:56.096121Z",
     "iopub.status.idle": "2025-10-02T19:46:56.102727Z",
     "shell.execute_reply": "2025-10-02T19:46:56.102154Z"
    },
    "papermill": {
     "duration": 0.011245,
     "end_time": "2025-10-02T19:46:56.103691",
     "exception": false,
     "start_time": "2025-10-02T19:46:56.092446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(size=32,padding=4),\n",
    "    transforms.ToDtype(torch.float32,scale=True),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),std=(0.247, 0.243, 0.261))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32,scale=True),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),std=(0.247, 0.243, 0.261))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "537cee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import default_collate\n",
    "\n",
    "cutmix = transforms.CutMix(num_classes=10)\n",
    "mixup = transforms.MixUp(num_classes=10)\n",
    "cutmix_or_mixup = transforms.RandomChoice([cutmix, mixup])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return cutmix_or_mixup(*default_collate(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e736d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:46:56.110649Z",
     "iopub.status.busy": "2025-10-02T19:46:56.110453Z",
     "iopub.status.idle": "2025-10-02T19:47:08.238701Z",
     "shell.execute_reply": "2025-10-02T19:47:08.238099Z"
    },
    "papermill": {
     "duration": 12.133251,
     "end_time": "2025-10-02T19:47:08.240087",
     "exception": false,
     "start_time": "2025-10-02T19:46:56.106836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join(cwd,\"./data\"),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=os.path.join(cwd,\"./data\"),\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330dc0c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.255515Z",
     "iopub.status.busy": "2025-10-02T19:47:08.254923Z",
     "iopub.status.idle": "2025-10-02T19:47:08.258560Z",
     "shell.execute_reply": "2025-10-02T19:47:08.257898Z"
    },
    "papermill": {
     "duration": 0.011735,
     "end_time": "2025-10-02T19:47:08.259683",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.247948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# train_subset = Subset(train_dataset, range(1000))\n",
    "train_subset = train_dataset\n",
    "# test_subset = Subset(test_dataset, range(10000))\n",
    "test_subset = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec38959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.273204Z",
     "iopub.status.busy": "2025-10-02T19:47:08.272610Z",
     "iopub.status.idle": "2025-10-02T19:47:08.276417Z",
     "shell.execute_reply": "2025-10-02T19:47:08.275879Z"
    },
    "papermill": {
     "duration": 0.011561,
     "end_time": "2025-10-02T19:47:08.277414",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.265853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_subset, batch_size=256, shuffle=True, num_workers = 0,collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_subset, batch_size=256, shuffle=False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1a616",
   "metadata": {
    "papermill": {
     "duration": 0.005849,
     "end_time": "2025-10-02T19:47:08.289370",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.283521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a70e388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.302586Z",
     "iopub.status.busy": "2025-10-02T19:47:08.302379Z",
     "iopub.status.idle": "2025-10-02T19:47:08.315263Z",
     "shell.execute_reply": "2025-10-02T19:47:08.314593Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.021058,
     "end_time": "2025-10-02T19:47:08.316456",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.295398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(feature_dim),\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, feature_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(feature_dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, feature_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class ShiftedPatchTokenization(nn.Module):\n",
    "    def __init__(self, *, feature_dim, patch_size, channels = 3):\n",
    "        super().__init__()\n",
    "        patch_dim = patch_size * patch_size * 5 * channels\n",
    "\n",
    "        self.to_patch_tokens = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
    "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
    "        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n",
    "        return self.to_patch_tokens(x_with_shifts)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, feature_dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(feature_dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(feature_dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, feature_dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = (image_size,image_size)\n",
    "        patch_height, patch_width = (patch_size,patch_size)\n",
    "\n",
    "        assert (image_height % patch_height == 0) and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        # patch_dim = channels * patch_height * patch_width\n",
    "        # self.to_patch_embedding = nn.Sequential(\n",
    "        #     Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "        #     nn.LayerNorm(patch_dim),\n",
    "        #     nn.Linear(patch_dim, feature_dim),\n",
    "        #     nn.LayerNorm(feature_dim),\n",
    "        # )\n",
    "\n",
    "        self.to_patch_embedding = ShiftedPatchTokenization(\n",
    "            feature_dim=feature_dim,\n",
    "            patch_size=patch_size,\n",
    "            channels=channels\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, feature_dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, feature_dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(feature_dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.mlp_head = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x[:, 0] # get the CLS token\n",
    "\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf154712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.329888Z",
     "iopub.status.busy": "2025-10-02T19:47:08.329613Z",
     "iopub.status.idle": "2025-10-02T19:47:08.332524Z",
     "shell.execute_reply": "2025-10-02T19:47:08.332011Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010793,
     "end_time": "2025-10-02T19:47:08.333446",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.322653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = ViT(\n",
    "#     image_size=32,\n",
    "#     num_classes=10,\n",
    "#     patch_size=4,\n",
    "#     feature_dim=256,\n",
    "#     mlp_dim=512,\n",
    "#     depth=6,\n",
    "#     heads=4,\n",
    "#     dropout=0.3,\n",
    "#     emb_dropout=0.3,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea33eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.346363Z",
     "iopub.status.busy": "2025-10-02T19:47:08.346173Z",
     "iopub.status.idle": "2025-10-02T19:47:08.349089Z",
     "shell.execute_reply": "2025-10-02T19:47:08.348576Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010417,
     "end_time": "2025-10-02T19:47:08.350026",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.339609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f3945a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.363273Z",
     "iopub.status.busy": "2025-10-02T19:47:08.363070Z",
     "iopub.status.idle": "2025-10-02T19:47:08.365935Z",
     "shell.execute_reply": "2025-10-02T19:47:08.365277Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010781,
     "end_time": "2025-10-02T19:47:08.367027",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.356246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# summary(model,input_size=(64,3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd215283",
   "metadata": {
    "papermill": {
     "duration": 0.006001,
     "end_time": "2025-10-02T19:47:08.379527",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.373526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d491cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.392759Z",
     "iopub.status.busy": "2025-10-02T19:47:08.392533Z",
     "iopub.status.idle": "2025-10-02T19:47:08.401742Z",
     "shell.execute_reply": "2025-10-02T19:47:08.401213Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.017098,
     "end_time": "2025-10-02T19:47:08.402712",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.385614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleLogger:\n",
    "    def __init__(self) -> None:\n",
    "        self.epoch = []\n",
    "        self.train_loss = []\n",
    "        self.train_acc = []\n",
    "        self.eval_loss = []\n",
    "        self.eval_acc = []\n",
    "        \n",
    "    def log(self, metrics):\n",
    "        self.epoch.append(metrics['epoch'])\n",
    "        self.train_loss.append(metrics['train_loss'])\n",
    "        self.train_acc.append(metrics['train_acc'])\n",
    "        self.eval_loss.append(metrics['eval_loss'])\n",
    "        self.eval_acc.append(metrics['eval_acc'])\n",
    "\n",
    "    def plot_metrics(self, save_path=None,display=False):\n",
    "        if not self.epoch:\n",
    "            print(\"No metrics to plot. Train the model first.\")\n",
    "            return\n",
    "        \n",
    "        epochs = self.epoch\n",
    "        train_losses = self.train_loss\n",
    "        eval_losses = self.eval_loss\n",
    "        train_accs = self.train_acc\n",
    "        eval_accs = self.eval_acc\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Plot loss curves\n",
    "        ax1.plot(epochs, train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "        ax1.plot(epochs, eval_losses, label='Eval Loss', marker='s', linewidth=2)\n",
    "        ax1.set_xlabel('Epochs', fontsize=12)\n",
    "        ax1.set_ylabel('Loss', fontsize=12)\n",
    "        ax1.set_title('Training and Evaluation Loss', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(loc='best')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot accuracy curves\n",
    "        ax2.plot(epochs, train_accs, label='Train Accuracy', marker='o', linewidth=2)\n",
    "        ax2.plot(epochs, eval_accs, label='Eval Accuracy', marker='s', linewidth=2)\n",
    "        ax2.set_xlabel('Epochs', fontsize=12)\n",
    "        ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax2.set_title('Training and Evaluation Accuracy', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(loc='best')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path is None and display==False:\n",
    "            raise AttributeError('Must provide either save path or option to display.')\n",
    "        else:\n",
    "            if save_path:\n",
    "                plt.savefig(save_path+\".png\", dpi=300, bbox_inches='tight')\n",
    "                print(f\"Plot saved to {save_path}\")\n",
    "            if display:\n",
    "                plt.show()\n",
    "    \n",
    "    def store_log_as_csv(self, save_path=None):\n",
    "        if save_path is None:\n",
    "            raise ValueError(\"Save path cannot be None\")\n",
    "        \n",
    "        save_path += '.csv'\n",
    "        \n",
    "        rows = []\n",
    "        rows.append([\"epoch\", \"train_loss\", \"train_acc\", \"eval_loss\", \"eval_acc\"])\n",
    "        \n",
    "        for i in range(len(self.epoch)):\n",
    "            rows.append([\n",
    "                self.epoch[i],\n",
    "                self.train_loss[i],\n",
    "                self.train_acc[i],\n",
    "                self.eval_loss[i],\n",
    "                self.eval_acc[i]\n",
    "            ])\n",
    "        \n",
    "        with open(save_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5f22f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T19:47:08.416025Z",
     "iopub.status.busy": "2025-10-02T19:47:08.415847Z",
     "iopub.status.idle": "2025-10-02T19:47:08.427406Z",
     "shell.execute_reply": "2025-10-02T19:47:08.426884Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.019415,
     "end_time": "2025-10-02T19:47:08.428403",
     "exception": false,
     "start_time": "2025-10-02T19:47:08.408988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        scheduler=None,\n",
    "        logger=None,\n",
    "        ckpt_path=\"checkpoint.pt\",\n",
    "        device=device,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.scheduler = scheduler\n",
    "        self.logger:SimpleLogger = logger\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.best_acc = 0.0\n",
    "        self.start_epoch = 0\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _save_checkpoint(self, epoch, acc):\n",
    "        state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state\": self.scheduler.state_dict() if self.scheduler else None,\n",
    "            \"best_acc\": acc,\n",
    "        }\n",
    "        torch.save(state, self.ckpt_path + '.pt')\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        if os.path.isfile(self.ckpt_path + '.pt'):\n",
    "            state = torch.load(self.ckpt_path + '.pt', map_location=self.device)\n",
    "            self.model.load_state_dict(state[\"model_state\"])\n",
    "            self.optimizer.load_state_dict(state[\"optimizer_state\"])\n",
    "            if self.scheduler and state.get(\"scheduler_state\"):\n",
    "                self.scheduler.load_state_dict(state[\"scheduler_state\"])\n",
    "            self.best_acc = state.get(\"best_acc\", 0.0)\n",
    "            self.start_epoch = state.get(\"epoch\", 0)\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(self.start_epoch, num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs}')\n",
    "            train_loss, train_acc = self._train_one_epoch(epoch)\n",
    "            eval_loss, eval_acc = self._evaluate(epoch)\n",
    "            if self.logger:\n",
    "                self.logger.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"eval_loss\": eval_loss,\n",
    "                    \"eval_acc\": eval_acc\n",
    "                })\n",
    "            if eval_acc > self.best_acc:\n",
    "                self.best_acc = eval_acc\n",
    "                print(\"Saving model...\")\n",
    "                self._save_checkpoint(epoch, eval_acc)\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f} | Best Eval Acc: {self.best_acc:.2f}\")\n",
    "        \n",
    "        self.logger.plot_metrics(save_path = self.ckpt_path)\n",
    "        self.logger.store_log_as_csv(self.ckpt_path)\n",
    "        return self.best_acc\n",
    "\n",
    "    def _train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        pbar = tqdm(self.train_loader, total=len(self.train_loader),desc=\"Training: \")\n",
    "        for batch in pbar:\n",
    "            x, y = batch[0].to(self.device), batch[1].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(x)\n",
    "\n",
    "            if torch.is_floating_point(y) and y.dim() == 2:\n",
    "                # y are soft labels\n",
    "                log_probs = F.log_softmax(outputs, dim=1)\n",
    "                # negative log-likelihood for soft labels\n",
    "                loss = -(y * log_probs).sum(dim=1).mean()\n",
    "            else:\n",
    "                # assume standard class indices\n",
    "                loss = self.loss_fn(outputs, y)\n",
    "            \n",
    "            loss = self.loss_fn(outputs, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # need to calculate accuracy differently for soft labels\n",
    "            labels = batch[1]\n",
    "            if torch.is_floating_point(labels) and labels.dim() == 2:\n",
    "                # compute accuracy by comparing argmax of soft labels\n",
    "                target_indices = labels.argmax(dim=1).to(self.device)\n",
    "                total_correct += (outputs.argmax(1) == target_indices).sum().item()\n",
    "            else:\n",
    "                total_correct += (outputs.argmax(1) == y).sum().item()\n",
    "            total_samples += x.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_acc = total_correct / total_samples\n",
    "        return avg_loss, avg_acc\n",
    "\n",
    "    def _evaluate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.test_loader, total=len(self.test_loader),desc=\"Evaluating: \")\n",
    "            for batch in pbar:\n",
    "                x, y = batch[0].to(self.device), batch[1].to(self.device)\n",
    "                outputs = self.model(x)\n",
    "                loss = self.loss_fn(outputs, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total_samples += x.size(0)\n",
    "        avg_loss = total_loss / total_samples\n",
    "        avg_acc = total_correct / total_samples\n",
    "        return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "image_size=32,\n",
    "num_classes=10,\n",
    "patch_size=4,\n",
    "feature_dim=768,\n",
    "mlp_dim=768*2,\n",
    "depth=12,\n",
    "heads=4,\n",
    "dropout=0.3,\n",
    "emb_dropout=0.3,\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=6e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.3\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "warmup_epochs = 30\n",
    "\n",
    "# Linear warmup scheduler\n",
    "warmup_scheduler = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.01,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_epochs\n",
    ")\n",
    "\n",
    "# Cosine annealing scheduler\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs - warmup_epochs,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_scheduler, cosine_scheduler],\n",
    "    milestones=[warmup_epochs]\n",
    ")\n",
    "\n",
    "logger = SimpleLogger()\n",
    "\n",
    "# Instantiate the Trainer\n",
    "save_name = f\"checkpoint_{get_timestamp()}_SPT\"\n",
    "print(f'Saving to {save_name}')\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=scheduler,\n",
    "    logger=logger,\n",
    "    ckpt_path=os.path.join(cwd,save_name),\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "best_acc = trainer.train(num_epochs=num_epochs)\n",
    "\n",
    "print(f'Best Acc: {best_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2080.624715,
   "end_time": "2025-10-02T20:21:21.240902",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-02T19:46:40.616187",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
